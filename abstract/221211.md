# 7 papers from arxiv about "AI for Math" from 30 Nov to 11 Dec

### A Survey of Question Answering for Math and Science Problem
**source**: arXiv:1705.04530 [[paper](https://arxiv.org/abs/1705.04530)]
**abstract**: Turing test was long considered the measure for artificial intelligence. Butwith the advances in AI, it has proved to be insufficient measure. We can nowaim to mea- sure machine intelligence like we measure human intelligence. Oneof the widely accepted measure of intelligence is standardized math and sciencetest. In this paper, we explore the progress we have made towards the goal ofmaking a machine smart enough to pass the standardized test. We see thechallenges and opportunities posed by the domain, and note that we are quitesome ways from actually making a system as smart as a even a middle schoolscholar.

### Peano: Learning Formal Mathematical Reasoning
**source**: arXiv:2211.15864 [[paper](https://arxiv.org/abs/2211.15864)]
**abstract**: General mathematical reasoning is computationally undecidable, but humansroutinely solve new problems. Moreover, discoveries developed over centuriesare taught to subsequent generations quickly. What structure enables this, andhow might that inform automated mathematical reasoning? We posit that centralto both puzzles is the structure of procedural abstractions underlyingmathematics. We explore this idea in a case study on 5 sections of beginningalgebra on the Khan Academy platform. To define a computational foundation, weintroduce Peano, a theorem-proving environment where the set of valid actionsat any point is finite. We use Peano to formalize introductory algebra problemsand axioms, obtaining well-defined search problems. We observe existingreinforcement learning methods for symbolic reasoning to be insufficient tosolve harder problems. Adding the ability to induce reusable abstractions("tactics") from its own solutions allows an agent to make steady progress,solving all problems. Furthermore, these abstractions induce an order to theproblems, seen at random during training. The recovered order has significantagreement with the expert-designed Khan Academy curriculum, andsecond-generation agents trained on the recovered curriculum learnsignificantly faster. These results illustrate the synergistic role ofabstractions and curricula in the cultural transmission of mathematics.

### Chaining Simultaneous Thoughts for Numerical Reasoning
**source**: arXiv:2211.16482 [[paper](https://arxiv.org/abs/2211.16482)]
**abstract**: Given that rich information is hidden behind ubiquitous numbers in text,numerical reasoning over text should be an essential skill of AI systems. Toderive precise equations to solve numerical reasoning problems, previous workfocused on modeling the structures of equations, and has proposed variousstructured decoders. Though structure modeling proves to be effective, thesestructured decoders construct a single equation in a pre-defined autoregressiveorder, potentially placing an unnecessary restriction on how a model shouldgrasp the reasoning process. Intuitively, humans may have numerous pieces ofthoughts popping up in no pre-defined order; thoughts are not limited to theproblem at hand, and can even be concerned with other related problems. Bycomparing diverse thoughts and chaining relevant pieces, humans are less proneto errors. In this paper, we take this inspiration and propose CANTOR, anumerical reasoner that models reasoning steps using a directed acyclic graphwhere we produce diverse reasoning steps simultaneously without pre-defineddecoding dependencies, and compare and chain relevant ones to reach a solution.Extensive experiments demonstrated the effectiveness of CANTOR under bothfully-supervised and weakly-supervised settings.

### Solving math word problems with process- and outcome-based feedback
**source**: arXiv:2211.14275 [[paper](https://arxiv.org/abs/2211.14275)]
**abstract**: Recent work has shown that asking language models to generate reasoning stepsimproves performance on many reasoning tasks. When moving beyond prompting,this raises the question of how we should supervise such models: outcome-basedapproaches which supervise the final result, or process-based approaches whichsupervise the reasoning process itself? Differences between these approachesmight naturally be expected not just in final-answer errors but also inreasoning errors, which can be difficult to detect and are problematic in manyreal-world domains such as education. We run the first comprehensive comparisonbetween process- and outcome-based approaches trained on a natural languagetask, GSM8K. We find that pure outcome-based supervision produces similarfinal-answer error rates with less label supervision. However, for correctreasoning steps we find it necessary to use process-based supervision orsupervision from learned reward models that emulate process-based feedback. Intotal, we improve the previous best results from 16.8% to 12.7% final-answererror and 14.0% to 3.4% reasoning error among final-answer-correctsolutions.

### Textual Enhanced Contrastive Learning for Solving Math Word Problems
**source**: arXiv:2211.16022 [[paper](https://arxiv.org/abs/2211.16022)]
**abstract**: Solving math word problems is the task that analyses the relation ofquantities and requires an accurate understanding of contextual naturallanguage information. Recent studies show that current models rely on shallowheuristics to predict solutions and could be easily misled by small textualperturbations. To address this problem, we propose a Textual EnhancedContrastive Learning framework, which enforces the models to distinguishsemantically similar examples while holding different mathematical logic. Weadopt a self-supervised manner strategy to enrich examples with subtle textualvariance by textual reordering or problem re-construction. We then retrieve thehardest to differentiate samples from both equation and textual perspectivesand guide the model to learn their representations. Experimental results showthat our method achieves state-of-the-art on both widely used benchmarkdatasets and also exquisitely designed challenge datasets in English andChinese. Our code and data is available at [here](https://github.com/yiyunya/Textual_CL_MWP).

### UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression
**source**: arXiv:2212.02746 [[paper](https://arxiv.org/abs/2212.02746)]
**abstract**: Geometry problem solving is a well-recognized testbed for evaluating thehigh-level multi-modal reasoning capability of deep models. In most existingworks, two main geometry problems: calculation and proving, are usually treatedas two specific tasks, hindering a deep model to unify its reasoning capabilityon multiple math tasks. However, in essence, these two tasks have similarproblem representations and overlapped math knowledge which can improve theunderstanding and reasoning ability of a deep model on both two tasks.Therefore, we construct a large-scale Unified Geometry problem benchmark,UniGeo, which contains 4,998 calculation problems and 9,543 proving problems.Each proving problem is annotated with a multi-step proof with reasons andmathematical expressions. The proof can be easily reformulated as a provingsequence that shares the same formats with the annotated program sequence forcalculation problems. Naturally, we also present a unified multi-task GeometricTransformer framework, Geoformer, to tackle calculation and proving problemssimultaneously in the form of sequence generation, which finally shows thereasoning ability can be improved on both two tasks by unifying formulation.Furthermore, we propose a Mathematical Expression Pretraining (MEP) method thataims to predict the mathematical expressions in the problem solution, thusimproving the Geoformer model. Experiments on the UniGeo demonstrate that ourproposed Geoformer obtains state-of-the-art performance by outperformingtask-specific model NGS with over 5.6% and 3.2% accuracies on calculation andproving problems, respectively.

### Analogical Math Word Problems Solving with Enhanced Problem-Solution Association
**source**: arXiv:2212.00837 [[paper](https://arxiv.org/abs/2212.00837)]
**abstract**: Math word problem (MWP) solving is an important task in question answeringwhich requires human-like reasoning ability. Analogical reasoning has long beenused in mathematical education, as it enables students to apply commonrelational structures of mathematical situations to solve new problems. In thispaper, we propose to build a novel MWP solver by leveraging analogical MWPs,which advance the solver's generalization ability across different kinds ofMWPs. The key idea, named analogy identification, is to associate theanalogical MWP pairs in a latent space, i.e., encoding an MWP close to anotheranalogical MWP, while moving away from the non-analogical ones. Moreover, asolution discriminator is integrated into the MWP solver to enhance theassociation between the representations of MWPs and their true solutions. Theevaluation results verify that our proposed analogical learning strategypromotes the performance of MWP-BERT on Math23k over the state-of-the-art modelGenerate2Rank, with 5 times fewer parameters in the encoder. We also find thatour model has a stronger generalization ability in solving difficult MWPs dueto the analogical learning from easy MWPs.

### Generalizing Math Word Problem Solvers via Solution Diversification
**source**: arXiv:2212.00833 [[paper](https://arxiv.org/abs/2212.00833)]
**abstract**: Current math word problem (MWP) solvers are usually Seq2Seq models trained bythe (one-problem; one-solution) pairs, each of which is made of a problemdescription and a solution showing reasoning flow to get the correct answer.However, one MWP problem naturally has multiple solution equations. Thetraining of an MWP solver with (one-problem; one-solution) pairs excludes othercorrect solutions, and thus limits the generalizability of the MWP solver. Onefeasible solution to this limitation is to augment multiple solutions to agiven problem. However, it is difficult to collect diverse and accurate augmentsolutions through human efforts. In this paper, we design a new trainingframework for an MWP solver by introducing a solution buffer and a solutiondiscriminator. The buffer includes solutions generated by an MWP solver toencourage the training data diversity. The discriminator controls the qualityof buffered solutions to participate in training. Our framework is flexiblyapplicable to a wide setting of fully, semi-weakly and weakly supervisedtraining for all Seq2Seq MWP solvers. We conduct extensive experiments on abenchmark dataset Math23k and a new dataset named Weak12k, and show that ourframework improves the performance of various MWP solvers under differentsettings by generating correct and diverse solutions.
