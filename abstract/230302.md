# 19 papers from arxiv about "AI for Math" from 20 Feb to 4 Mar

### On Robust Numerical Solver for ODE via Self-Attention Mechanism
**source**: arXiv:2302.10184 [[paper](https://arxiv.org/abs/2302.10184)]
**abstract**: With the development of deep learning techniques, AI-enhanced numericalsolvers are expected to become a new paradigm for solving differentialequations due to their versatility and effectiveness in alleviating theaccuracy-speed trade-off in traditional numerical solvers. However, thisparadigm still inevitably requires a large amount of high-quality data, whoseacquisition is often very expensive in natural science and engineeringproblems. Therefore, in this paper, we explore training efficient and robustAI-enhanced numerical solvers with a small data size by mitigating intrinsicnoise disturbances. We first analyze the ability of the self-attentionmechanism to regulate noise in supervised learning and then propose asimple-yet-effective numerical solver, AttSolver, which introduces an additiveself-attention mechanism to the numerical solution of differential equationsbased on the dynamical system perspective of the residual neural network. Ourresults on benchmarks, ranging from high-dimensional problems to chaoticsystems, demonstrate the effectiveness of AttSolver in generally improving theperformance of existing traditional numerical solvers without any elaboratedmodel crafting. Finally, we analyze the convergence, generalization, androbustness of the proposed method experimentally and theoretically.

### A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning
**source**: arXiv:2302.09068 [[paper](https://arxiv.org/abs/2302.09068)]
**abstract**: We conduct a pilot study selectively evaluating the cognitive abilities(decision making and spatial reasoning) of two recently released generativetransformer models, ChatGPT and DALL-E 2. Input prompts were constructedfollowing neutral a priori guidelines, rather than adversarial intent. Post hocqualitative analysis of the outputs shows that DALL-E 2 is able to generate atleast one correct image for each spatial reasoning prompt, but most imagesgenerated are incorrect (even though the model seems to have a clearunderstanding of the objects mentioned in the prompt). Similarly, in evaluatingChatGPT on the rationality axioms developed under the classical VonNeumann-Morgenstern utility theorem, we find that, although it demonstratessome level of rational decision-making, many of its decisions violate at leastone of the axioms even under reasonable constructions of preferences, bets, anddecision-making prompts. ChatGPT's outputs on such problems generally tended tobe unpredictable: even as it made irrational decisions (or employed anincorrect reasoning process) for some simpler decision-making problems, it wasable to draw correct conclusions for more complex bet structures. We brieflycomment on the nuances and challenges involved in scaling up such a 'cognitive'evaluation or conducting it with a closed set of answer keys ('ground truth'),given that these models are inherently generative and open-ended in respondingto prompts.

### Efficient Generator of Mathematical Expressions for Symbolic Regression
**source**: arXiv:2302.09893 [[paper](https://arxiv.org/abs/2302.09893)]
**abstract**: We propose an approach to symbolic regression based on a novel variationalautoencoder for generating hierarchical structures, HVAE. It combines simpleatomic units with shared weights to recursively encode and decode theindividual nodes in the hierarchy. Encoding is performed bottom-up and decodingtop-down. We empirically show that HVAE can be trained efficiently with smallcorpora of mathematical expressions and can accurately encode expressions intoa smooth low-dimensional latent space. The latter can be efficiently exploredwith various optimization methods to address the task of symbolic regression.Indeed, random search through the latent space of HVAE performs better thanrandom search through expressions generated by manually crafted probabilisticgrammars for mathematical expressions. Finally, EDHiE system for symbolicregression, which applies an evolutionary algorithm to the latent space ofHVAE, reconstructs equations from a standard symbolic regression benchmarkbetter than a state-of-the-art system based on a similar combination of deeplearning and evolutionary algorithms.Å¾

### Neuro-symbolic Meta Reinforcement Learning for Trading
**source**: arXiv:2302.08996 [[paper](https://arxiv.org/abs/2302.08996)]
**abstract**: We model short-duration (e.g. day) trading in financial markets as asequential decision-making problem under uncertainty, with the addedcomplication of continual concept-drift. We, therefore, employ metareinforcement learning via the RL2 algorithm. It is also known that humantraders often rely on frequently occurring symbolic patterns in price series.We employ logical program induction to discover symbolic patterns that occurfrequently as well as recently, and explore whether using such featuresimproves the performance of our meta reinforcement learning algorithm. Wereport experiments on real data indicating that meta-RL is better than vanillaRL and also benefits from learned symbolic features.

### Neural Algorithmic Reasoning with Causal Regularisation
**source**: arXiv:2302.10258 [[paper](https://arxiv.org/abs/2302.10258)]
**abstract**: Recent work on neural algorithmic reasoning has investigated the reasoningcapabilities of neural networks, effectively demonstrating they can learn toexecute classical algorithms on unseen data coming from the train distribution.However, the performance of existing neural reasoners significantly degrades onout-of-distribution (OOD) test data, where inputs have larger sizes. In thiswork, we make an important observation: there are many \emph{different} inputsfor which an algorithm will perform certain intermediate computations\emph{identically}. This insight allows us to develop data augmentationprocedures that, given an algorithm's intermediate trajectory, produce inputsfor which the target algorithm would have \emph{exactly} the same nexttrajectory step. Then, we employ a causal framework to design a correspondingself-supervised objective, and we prove that it improves the OOD generalisationcapabilities of the reasoner. We evaluate our method on the CLRS algorithmicreasoning benchmark, where we show up to 3$\times$ improvements on the OOD testdata.

### BERT is not The Count: Learning to Match Mathematical Statements with Proofs
**source**: arXiv:2302.09350 [[paper](https://arxiv.org/abs/2302.09350)]
**abstract**: We introduce a task consisting in matching a proof to a given mathematicalstatement. The task fits well within current research on MathematicalInformation Retrieval and, more generally, mathematical article analysis(Mathematical Sciences, 2014). We present a dataset for the task (the MATcHdataset) consisting of over 180k statement-proof pairs extracted from modernmathematical research articles. We find this dataset highly representative ofour task, as it consists of relatively new findings useful to mathematicians.We propose a bilinear similarity model and two decoding methods to matchstatements to proofs effectively. While the first decoding method matches aproof to a statement without being aware of other statements or proofs, thesecond method treats the task as a global matching problem. Through a symbolreplacement procedure, we analyze the "insights" that pre-trained languagemodels have in such mathematical article analysis and show that while thesemodels perform well on this task with the best performing mean reciprocal rankof 73.7, they follow a relatively shallow symbolic analysis and matching toachieve that performance.

### Online Symbolic Regression with Informative Query
**source**: arXiv:2302.10539 [[paper](https://arxiv.org/abs/2302.10539)]
**abstract**: Symbolic regression, the task of extracting mathematical expressions from theobserved data $\{ \vx_i, y_i \}$, plays a crucial role in scientific discovery.Despite the promising performance of existing methods, most of them conductsymbolic regression in an \textit{offline} setting. That is, they treat theobserved data points as given ones that are simply sampled from uniformdistributions without exploring the expressive potential of data. However, forreal-world scientific problems, the data used for symbolic regression areusually actively obtained by doing experiments, which is an \textit{online}setting. Thus, how to obtain informative data that can facilitate the symbolicregression process is an important problem that remains challenging.  In this paper, we propose QUOSR, a \textbf{qu}ery-based framework for\textbf{o}nline \textbf{s}ymbolic \textbf{r}egression that can automaticallyobtain informative data in an iterative manner. Specifically, at each step,QUOSR receives historical data points, generates new $\vx$, and then queriesthe symbolic expression to get the corresponding $y$, where the $(\vx, y)$serves as new data points. This process repeats until the maximum number ofquery steps is reached. To make the generated data points informative, weimplement the framework with a neural network and train it by maximizing themutual information between generated data points and the target expression.Through comprehensive experiments, we show that QUOSR can facilitate modernsymbolic regression methods by generating informative data.

### NeuralStagger: accelerating physics-constrained neural PDE solver with spatial-temporal decomposition
**source**: arXiv:2302.10255 [[paper](https://arxiv.org/abs/2302.10255)]
**abstract**: Neural networks have shown great potential in accelerating the solution ofpartial differential equations (PDEs). Recently, there has been a growinginterest in introducing physics constraints into training neural PDE solvers toreduce the use of costly data and improve the generalization ability. However,these physics constraints, based on certain finite dimensional approximationsover the function space, must resolve the smallest scaled physics to ensure theaccuracy and stability of the simulation, resulting in high computational costsfrom large input, output, and neural networks. This paper proposes a generalacceleration methodology called NeuralStagger by spatially and temporallydecomposing the original learning tasks into several coarser-resolutionsubtasks. We define a coarse-resolution neural solver for each subtask, whichrequires fewer computational resources, and jointly train them with the vanillaphysics-constrained loss by simply arranging their outputs to reconstruct theoriginal solution. Due to the perfect parallelism between them, the solution isachieved as fast as a coarse-resolution neural solver. In addition, the trainedsolvers bring the flexibility of simulating with multiple levels of resolution.We demonstrate the successful application of NeuralStagger on 2D and 3D fluiddynamics simulations, which leads to an additional $10\sim100\times$ speed-up.Moreover, the experiment also shows that the learned model could be well usedfor optimal control.

### BERT is not The Count: Learning to Match Mathematical Statements with Proofs
**source**: arXiv:2302.09350 [[paper](https://arxiv.org/abs/2302.09350)]
**abstract**: We introduce a task consisting in matching a proof to a given mathematicalstatement. The task fits well within current research on MathematicalInformation Retrieval and, more generally, mathematical article analysis(Mathematical Sciences, 2014). We present a dataset for the task (the MATcHdataset) consisting of over 180k statement-proof pairs extracted from modernmathematical research articles. We find this dataset highly representative ofour task, as it consists of relatively new findings useful to mathematicians.We propose a bilinear similarity model and two decoding methods to matchstatements to proofs effectively. While the first decoding method matches aproof to a statement without being aware of other statements or proofs, thesecond method treats the task as a global matching problem. Through a symbolreplacement procedure, we analyze the "insights" that pre-trained languagemodels have in such mathematical article analysis and show that while thesemodels perform well on this task with the best performing mean reciprocal rankof 73.7, they follow a relatively shallow symbolic analysis and matching toachieve that performance.

### Semantic Strengthening of Neuro-Symbolic Learning
**source**: arXiv:2302.14207 [[paper](https://arxiv.org/abs/2302.14207)]
**abstract**: Numerous neuro-symbolic approaches have recently been proposed typically withthe goal of adding symbolic knowledge to the output layer of a neural network.Ideally, such losses maximize the probability that the neural network'spredictions satisfy the underlying domain. Unfortunately, this type ofprobabilistic inference is often computationally infeasible. Neuro-symbolicapproaches therefore commonly resort to fuzzy approximations of thisprobabilistic objective, sacrificing sound probabilistic semantics, or tosampling which is very seldom feasible. We approach the problem by firstassuming the constraint decomposes conditioned on the features learned by thenetwork. We iteratively strengthen our approximation, restoring the dependencebetween the constraints most responsible for degrading the quality of theapproximation. This corresponds to computing the mutual information betweenpairs of constraints conditioned on the network's learned features, and may beconstrued as a measure of how well aligned the gradients of two distributionsare. We show how to compute this efficiently for tractable circuits. We testour approach on three tasks: predicting a minimum-cost path in Warcraft,predicting a minimum-cost perfect matching, and solving Sudoku puzzles,observing that it improves upon the baselines while sidesteppingintractability.

### Does a Neural Network Really Encode Symbolic Concept?
**source**: arXiv:2302.13080 [[paper](https://arxiv.org/abs/2302.13080)]
**abstract**: Recently, a series of studies have tried to extract interactions betweeninput variables modeled by a DNN and define such interactions as conceptsencoded by the DNN. However, strictly speaking, there still lacks a solidguarantee whether such interactions indeed represent meaningful concepts.Therefore, in this paper, we examine the trustworthiness of interactionconcepts from four perspectives. Extensive empirical studies have verified thata well-trained DNN usually encodes sparse, transferable, and discriminativeconcepts, which is partially aligned with human intuition.

### An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)
**source**: arXiv:2302.13814 [[paper](https://arxiv.org/abs/2302.13814)]
**abstract**: We study the performance of a commercially available large language model(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.To our knowledge, this is the first independent evaluation of ChatGPT. We foundthat ChatGPT's performance changes dramatically based on the requirement toshow its work, failing 20% of the time when it provides work compared with 84%when it does not. Further several factors about MWPs relating to the number ofunknowns and number of operations that lead to a higher probability of failurewhen compared with the prior, specifically noting (across all experiments) thatthe probability of failure increases linearly with the number of addition andsubtraction operations. We also have released the dataset of ChatGPT'sresponses to the MWPs to support further work on the characterization of LLMperformance and present baseline machine learning models to predict if ChatGPTcan correctly answer an MWP. We have released a dataset comprised of ChatGPT'sresponses to support further research in this area.

### ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics
**source**: arXiv:2302.12433 [[paper](https://arxiv.org/abs/2302.12433)]
**abstract**: We introduce ProofNet, a benchmark for autoformalization and formal provingof undergraduate-level mathematics. The ProofNet benchmarks consists of 371examples, each consisting of a formal theorem statement in Lean 3, a naturallanguage theorem statement, and a natural language proof. The problems areprimarily drawn from popular undergraduate pure mathematics textbooks and covertopics such as real and complex analysis, linear algebra, abstract algebra, andtopology. We intend for ProofNet to be a challenging benchmark that will driveprogress in autoformalization and automatic theorem proving. We report baselineresults on statement autoformalization via in-context learning. Moreover, weintroduce two novel statement autoformalization methods: prompt retrieval anddistilled backtranslation.

### Implicit Temporal Reasoning for Evidence-Based Fact-Checking
**source**: arXiv:2302.12569 [[paper](https://arxiv.org/abs/2302.12569)]
**abstract**: Leveraging contextual knowledge has become standard practice in automatedclaim verification, yet the impact of temporal reasoning has been largelyoverlooked. Our study demonstrates that time positively influences the claimverification process of evidence-based fact-checking. The temporal aspects andrelations between claims and evidence are first established through groundingon shared timelines, which are constructed using publication dates and timeexpressions extracted from their text. Temporal information is then provided toRNN-based and Transformer-based classifiers before or after claim and evidenceencoding. Our time-aware fact-checking models surpass base models by up to 9%Micro F1 (64.17%) and 15% Macro F1 (47.43%) on the MultiFC dataset. They alsooutperform prior methods that explicitly model temporal relations betweenevidence. Our findings show that the presence of temporal information and themanner in which timelines are constructed greatly influence how fact-checkingmodels determine the relevance and supporting or refuting character of evidencedocuments.

### Fusion of ML with numerical simulation for optimized propeller design
**source**: arXiv:2302.14740 [[paper](https://arxiv.org/abs/2302.14740)]
**abstract**: In computer-aided engineering design, the goal of a designer is to find anoptimal design on a given requirement using the numerical simulator in loopwith an optimization method. In this design optimization process, a good designoptimization process is one that can reduce the time from inception to design.In this work, we take a class of design problem, that is computationally cheapto evaluate but has high dimensional design space. In such cases, traditionalsurrogate-based optimization does not offer any benefits. In this work, wepropose an alternative way to use ML model to surrogate the design process thatformulates the search problem as an inverse problem and can save time byfinding the optimal design or at least a good initial seed design foroptimization. By using this trained surrogate model with the traditionaloptimization method, we can get the best of both worlds. We call this asSurrogate Assisted Optimization (SAO)- a hybrid approach by mixing ML surrogatewith the traditional optimization method. Empirical evaluations of propellerdesign problems show that a better efficient design can be found in fewerevaluations using SAO.

### Invariant Neural Ordinary Differential Equations
**source**: arXiv:2302.13262 [[paper](https://arxiv.org/abs/2302.13262)]
**abstract**: Latent neural ordinary differential equations have been proven useful forlearning non-linear dynamics of arbitrary sequences. In contrast with theirmechanistic counterparts, the predictive accuracy of neural ODEs decreases overlonger prediction horizons (Rubanova et al., 2019). To mitigate this issue, wepropose disentangling dynamic states from time-invariant variables in acompletely data-driven way, enabling robust neural ODE models that cangeneralize across different settings. We show that such variables can controlthe latent differential function and/or parameterize the mapping from latentvariables to observations. By explicitly modeling the time-invariant variables,our framework enables the use of recent advances in representation learning. Wedemonstrate this by introducing a straightforward self-supervised objectivethat enhances the learning of these variables. The experiments onlow-dimensional oscillating systems and video sequences reveal that ourdisentangled model achieves improved long-term predictions, when the trainingdata involve sequence-specific factors of variation such as differentrotational speeds, calligraphic styles, and friction constants.

### Solving differential equations using physics informed deep learning: a hand-on tutorial with benchmark tests
**source**: arXiv:2302.12260 [[paper](https://arxiv.org/abs/2302.12260)]
**abstract**: We revisit the original approach of using deep learning and neural networksto solve differential equations by incorporating the knowledge of the equation.This is done by adding a dedicated term to the loss function during theoptimization procedure in the training process. The so-called physics-informedneural networks (PINNs) are tested on a variety of academic ordinarydifferential equations in order to highlight the benefits and drawbacks of thisapproach with respect to standard integration methods. We focus on thepossibility to use the least possible amount of data into the training process.The principles of PINNs for solving differential equations by enforcingphysical laws via penalizing terms are reviewed. A tutorial on a simpleequation model illustrates how to put into practice the method for ordinarydifferential equations. Benchmark tests show that a very small amount oftraining data is sufficient to predict the solution when the non linearity ofthe problem is weak. However, this is not the case in strongly non linearproblems where a priori knowledge of training data over some partial or thewhole time integration interval is necessary.

### Q-Flow: Generative Modeling for Differential Equations of Open Quantum Dynamics with Normalizing Flows
**source**: arXiv:2302.12235 [[paper](https://arxiv.org/abs/2302.12235)]
**abstract**: Studying the dynamics of open quantum systems holds the potential to enablebreakthroughs both in fundamental physics and applications to quantumengineering and quantum computation. Due to the high-dimensional nature of theproblem, customized deep generative neural networks have been instrumental inmodeling the high-dimensional density matrix $Ï$, which is the keydescription for the dynamics of such systems. However, the complex-valuednature and normalization constraints of $Ï$, as well as its complicateddynamics, prohibit a seamless connection between open quantum systems and therecent advances in deep generative modeling. Here we lift that limitation byutilizing a reformulation of open quantum system dynamics to a partialdifferential equation (PDE) for a corresponding probability distribution $Q$,the Husimi Q function. Thus, we model the Q function seamlessly withoff-the-shelf deep generative models such as normalizing flows. Additionally,we develop novel methods for learning normalizing flow evolution governed byhigh-dimensional PDEs, based on the Euler method and the application of thetime-dependent variational principle. We name the resulting approach Q-Flow anddemonstrate the scalability and efficiency of Q-Flow on open quantum systemsimulations, including the dissipative harmonic oscillator and the dissipativebosonic model. Q-Flow is superior to conventional PDE solvers andstate-of-the-art physics-informed neural network solvers, especially inhigh-dimensional systems.

### Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search
**source**: arXiv:2302.11223 [[paper](https://arxiv.org/abs/2302.11223)]
**abstract**: Symbolic regression (SR) is the problem of learning a symbolic expressionfrom numerical data. Recently, deep neural models trained onprocedurally-generated synthetic datasets showed competitive performancecompared to more classical Genetic Programming (GP) algorithms. Unlike their GPcounterparts, these neural approaches are trained to generate expressions fromdatasets given as context. This allows them to produce accurate expressions ina single forward pass at test time. However, they usually do not benefit fromsearch abilities, which result in low performance compared to GP onout-of-distribution datasets. In this paper, we propose a novel method whichprovides the best of both worlds, based on a Monte-Carlo Tree Search procedureusing a context-aware neural mutation model, which is initially pre-trained tolearn promising mutations, and further refined from successful experiences inan online fashion. The approach demonstrates state-of-the-art performance onthe well-known \texttt{SRBench} benchmark.

