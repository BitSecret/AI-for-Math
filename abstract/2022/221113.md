# 1 paper from arxiv about "AI for Math" from 1 Nov to 13 Nov

### What is my math transformer doing? -- Three results on interpretability and generalization
**source**: arXiv:2211.00170 [[paper](https://arxiv.org/abs/2211.00170)]
**abstract**: This paper investigates the failure cases and out-of-distribution behavior oftransformers trained on matrix inversion and eigenvalue decomposition. I showthat incorrect model predictions still retain deep mathematical properties ofthe solution (e.g. correct eigenvalues, unit norm of eigenvectors), and thatalmost all model failures can be attributed to, and predicted from, propertiesof the problem or solution. This demonstrates that, when in doubt, mathtransformers do not hallucinate absurd solutions (as was sometimes proposed)but remain *roughly right*. I also show that the careful choice of a trainingdataset can accelerate training, while allowing the model to generalize out ofits training distribution, invalidating the idea that transformers *merelyinterpolate* from memorized examples.
