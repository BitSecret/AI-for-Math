# 3 papers from arxiv about "AI for Math" from 21 Nov to 24 Nov

### OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type
**source**: arXiv:2211.12164 [[paper](https://arxiv.org/abs/2211.12164)]
**abstract**: Machine generation of Arithmetic Word Problems (AWPs) is challenging as theyexpress quantities and mathematical relationships and need to be consistent.ML-solvers require a large annotated training set of consistent problems withlanguage variations. Exploiting domain-knowledge is needed for consistencychecking whereas LSTM-based approaches are good for producing text withlanguage variations. Combining these we propose a system, OLGA, to generateconsistent word problems of TC (Transfer-Case) type, involving object transfersamong agents. Though we provide a dataset of consistent 2-agent TC-problems fortraining, only about 36% of the outputs of an LSTM-based generator are foundconsistent. We use an extension of TC-Ontology, proposed by us previously, todetermine the consistency of problems. Among the remaining 64%, about 40% haveminor errors which we repair using the same ontology. To check consistency andfor the repair process, we construct an instance-specific representation (ABox)of an auto-generated problem. We use a sentence classifier and BERT models forthis task. The training set for these LMs is problem-texts where sentence-partsare annotated with ontology class-names. As three-agent problems are longer,the percentage of consistent problems generated by an LSTM-based approach dropsfurther. Hence, we propose an ontology-based method that extends consistent2-agent problems into consistent 3-agent problems. Overall, our approachgenerates a large number of consistent TC-type AWPs involving 2 or 3 agents. AsABox has all the information of a problem, any annotations can also begenerated. Adopting the proposed approach to generate other types of AWPs isinteresting future work.

### Automatic Generation of Socratic Subquestions for Teaching Math Word Problems
**source**: arXiv:2211.12835 [[paper](https://arxiv.org/abs/2211.12835)]
**abstract**: Socratic questioning is an educational method that allows students todiscover answers to complex problems by asking them a series of thoughtfulquestions. Generation of didactically sound questions is challenging, requiringunderstanding of the reasoning process involved in the problem. We hypothesizethat such questioning strategy can not only enhance the human performance, butalso assist the math word problem (MWP) solvers. In this work, we explore theability of large language models (LMs) in generating sequential questions forguiding math word problem-solving. We propose various guided questiongeneration schemes based on input conditioning and reinforcement learning. Onboth automatic and human quality evaluations, we find that LMs constrained withdesirable question properties generate superior questions and improve theoverall performance of a math word problem solver. We conduct a preliminaryuser study to examine the potential value of such question generation models inthe education domain. Results suggest that the difficulty level of problemsplays an important role in determining whether questioning improves or hindershuman performance. We discuss the future of using such questioning strategiesin education.

### DyRRen: A Dynamic Retriever-Reranker-Generator Model for Numerical Reasoning over Tabular and Textual Data
**source**: arXiv:2211.12668 [[paper](https://arxiv.org/abs/2211.12668)]
**abstract**: Numerical reasoning over hybrid data containing tables and long texts hasrecently received research attention from the AI community. To generate anexecutable reasoning program consisting of math and table operations to answera question, state-of-the-art methods use a retriever-generator pipeline.However, their retrieval results are static, while different generation stepsmay rely on different sentences. To attend to the retrieved information that isrelevant to each generation step, in this paper, we propose DyRRen, an extendedretriever-reranker-generator framework where each generation step is enhancedby a dynamic reranking of retrieved sentences. It outperforms existingbaselines on the FinQA dataset.
