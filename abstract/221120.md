# 4 papers from arxiv about "AI for Math" from 14 Nov to 20 Nov

### Semantic Representations of Mathematical Expressions in a Continuous Vector Space
**source**: arXiv:2211.08142 [[paper](https://arxiv.org/abs/2211.08142)]
**abstract**: Mathematical notation makes up a large portion of STEM literature, yet,finding semantic representations for formulae remains a challenging problem.Because mathematical notation is precise and its meaning changes significantlywith small character shifts, the methods that work for natural text do notnecessarily work well for mathematical expressions. In this work, we describean approach for representing mathematical expressions in a continuous vectorspace. We use the encoder of a sequence-to-sequence architecture, trained onvisually different but mathematically equivalent expressions, to generatevector representations (embeddings). We compare this approach with anautoencoder and show that the former is better at capturing mathematicalsemantics. Finally, to expedite future projects, we publish a corpus ofequivalent transcendental and algebraic expression pairs.

### Towards a Mathematics Formalisation Assistant using Large Language Models
**source**: arXiv:2211.07524 [[paper](https://arxiv.org/abs/2211.07524)]
**abstract**: Mathematics formalisation is the task of writing mathematics (i.e.,definitions, theorem statements, proofs) in natural language, as found in booksand papers, into a formal language that can then be checked for correctness bya program. It is a thriving activity today, however formalisation remainscumbersome. In this paper, we explore the abilities of a large language model(Codex) to help with formalisation in the Lean theorem prover. We find thatwith careful input-dependent prompt selection and postprocessing, Codex is ableto formalise short mathematical statements at undergrad level with nearly 75\%accuracy for $120$ theorem statements. For proofs quantitative analysis isinfeasible and we undertake a detailed case study. We choose a diverse set of$13$ theorems at undergrad level with proofs that fit in two-three paragraphs.We show that with a new prompting strategy Codex can formalise these proofs innatural language with at least one out of twelve Codex completion being easy torepair into a complete proof. This is surprising as essentially no aligned dataexists for formalised mathematics, particularly for proofs. These resultssuggest that large language models are a promising avenue towards fully orpartially automating formalisation.

### Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of NLP Systems
**source**: arXiv:2211.07455 [[paper](https://arxiv.org/abs/2211.07455)]
**abstract**: Numerical Question Answering is the task of answering questions that requirenumerical capabilities. Previous works introduce general adversarial attacks toNumerical Question Answering, while not systematically exploring numericalcapabilities specific to the topic. In this paper, we propose to conductnumerical capability diagnosis on a series of Numerical Question Answeringsystems and datasets. A series of numerical capabilities are highlighted, andcorresponding dataset perturbations are designed. Empirical results indicatethat existing systems are severely challenged by these perturbations. E.g.,Graph2Tree experienced a 53.83% absolute accuracy drop against the ``Extra''perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the``Language'' perturbation on the numerical subset of DROP. As a counteractingapproach, we also investigate the effectiveness of applying perturbations asdata augmentation to relieve systems' lack of robust numerical capabilities.With experiment analysis and empirical studies, it is demonstrated thatNumerical Question Answering with robust numerical capabilities is still to alarge extent an open question. We discuss future directions of NumericalQuestion Answering and summarize guidelines on future dataset collection andsystem design.

### LEMMA: Bootstrapping High-Level Mathematical Reasoning with Learned Symbolic Abstractions
**source**: arXiv:2211.08671 [[paper](https://arxiv.org/abs/2211.08671)]
**abstract**: Humans tame the complexity of mathematical reasoning by developinghierarchies of abstractions. With proper abstractions, solutions to hardproblems can be expressed concisely, thus making them more likely to be found.In this paper, we propose Learning Mathematical Abstractions (LEMMA): analgorithm that implements this idea for reinforcement learning agents inmathematical domains. LEMMA augments Expert Iteration with an abstraction step,where solutions found so far are revisited and rewritten in terms of newhigher-level actions, which then become available to solve new problems. Weevaluate LEMMA on two mathematical reasoning tasks--equation solving andfraction simplification--in a step-by-step fashion. In these two domains, LEMMAimproves the ability of an existing agent, both solving more problems andgeneralizing more effectively to harder problems than those seen duringtraining.
