# 4 papers from arxiv about "AI for Math" from 13 Dec to 31 Dec

### Automated Search for Conjectures on Mathematical Constants using Analysis of Integer Sequences
**source**: arXiv:2212.09470 [[paper](https://arxiv.org/abs/2212.09470)]  
**abstract**: Formulas involving fundamental mathematical constants had a great impact onvarious fields of science and mathematics, for example aiding in proofs ofirrationality of constants. However, the discovery of such formulas hashistorically remained scarce, often perceived as an act of mathematical geniusby great mathematicians such as Ramanujan, Euler, and Gauss. Recent efforts toautomate the discovery of formulas for mathematical constants, such as theRamanujan Machine project, relied on exhaustive search. Despite severalsuccessful discoveries, exhaustive search remains limited by the space ofoptions that can be covered and by the need for vast amounts of computationalresources. Here we propose a fundamentally different method to search forconjectures on mathematical constants: through analysis of integer sequences.We introduce the Enumerated Signed-continued-fraction Massey Approve (ESMA)algorithm, which builds on the Berlekamp-Massey algorithm to identify patternsin integer sequences that represent mathematical constants. The ESMA algorithmfound various known formulas for $e, e^2, tan(1)$, and ratios of values ofBessel functions. The algorithm further discovered a large number of newconjectures for these constants, some providing simpler representations andsome providing faster numerical convergence than the corresponding simplecontinued fractions. Along with the algorithm, we present mathematical toolsfor manipulating continued fractions. These connections enable us tocharacterize what space of constants can be found by ESMA and quantify itsalgorithmic advantage in certain scenarios. Altogether, this work continues inthe development of augmenting mathematical intuition by computer algorithms, tohelp reveal mathematical structures and accelerate mathematical research.

### MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering
**source**: arXiv:2212.09662 [[paper](https://arxiv.org/abs/2212.09662)]  
**abstract**: Visual language data such as plots, charts, and infographics are ubiquitousin the human world. However, state-of-the-art vision-language models do notperform well on these data. We propose MatCha (Math reasoning and Chartderendering pretraining) to enhance visual language models' capabilities injointly modeling charts/plots and language data. Specifically, we proposeseveral pretraining tasks that cover plot deconstruction and numericalreasoning which are the key capabilities in visual language modeling.  We perform the MatCha pretraining starting from Pix2Struct, a recentlyproposed image-to-text visual language model. On standard benchmarks such asPlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by asmuch as nearly 20%. We also examine how well MatCha pretraining transfers todomains such as screenshots, textbook diagrams, and document figures andobserve overall improvement, verifying the usefulness of MatCha pretraining onbroader visual language tasks.

### A Survey of Deep Learning for Mathematical Reasoning
**source**: arXiv:2212.10535 [[paper](https://arxiv.org/abs/2212.10535)]  
**abstract**: Mathematical reasoning is a fundamental aspect of human intelligence and isapplicable in various fields, including science, engineering, finance, andeveryday life. The development of artificial intelligence (AI) systems capableof solving math problems and proving theorems has garnered significant interestin the fields of machine learning and natural language processing. For example,mathematics serves as a testbed for aspects of reasoning that are challengingfor powerful deep learning models, driving new algorithmic and modelingadvances. On the other hand, recent advances in large-scale neural languagemodels have opened up new benchmarks and opportunities to use deep learning formathematical reasoning. In this survey paper, we review the key tasks,datasets, and methods at the intersection of mathematical reasoning and deeplearning over the past decade. We also evaluate existing benchmarks andmethods, and discuss future research directions in this domain.

### New Designed Loss Functions to Solve Ordinary Differential Equations with Artificial Neural Network
**source**: arXiv:2301.00636 [[paper](https://arxiv.org/abs/2301.00636)]  
**abstract**: This paper investigates the use of artificial neural networks (ANNs) to solvedifferential equations (DEs) and the construction of the loss function whichmeets both differential equation and its initial/boundary condition of acertain DE. In section 2, the loss function is generalized to $n^\text{th}$order ordinary differential equation(ODE). Other methods of construction areexamined in Section 3 and applied to three different models to assess theireffectiveness.
